{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matlab.engine as engi\n",
    "import matlab as mat\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,roc_auc_score,recall_score,precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from src import SMOTE\n",
    "from src import CFS\n",
    "from src import metrices\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(project):\n",
    "    understand_path = 'data/understand_files_all/' + project + '_understand.csv'\n",
    "    commit_guru_path = 'data/commit_guru/' + project + '.csv'\n",
    "    understand_df = pd.read_csv(understand_path)\n",
    "    understand_df = understand_df.dropna(axis = 1,how='all')\n",
    "    cols_list = understand_df.columns.values.tolist()\n",
    "#     print(cols_list)\n",
    "    for item in ['Kind', 'Name','commit_hash', 'Bugs']:\n",
    "        if item in cols_list:\n",
    "            cols_list.remove(item)\n",
    "            cols_list.insert(0,item)\n",
    "    understand_df = understand_df[cols_list]\n",
    "    commit_guru_df = pd.read_csv(commit_guru_path)\n",
    "    cols = understand_df.columns.tolist()\n",
    "    \n",
    "    commit_guru_df = commit_guru_df.drop(labels = ['parent_hashes','author_name','author_name',\n",
    "                                                   'author_email','fileschanged','author_date',\n",
    "                                                   'author_date_unix_timestamp', 'commit_message',\n",
    "                                                  'classification', 'fix', 'contains_bug','fixes',],axis=1)\n",
    "\n",
    "#     print(commit_guru_df.columns)\n",
    "    understand_df = understand_df.drop_duplicates(cols[4:len(cols)])\n",
    "    df = understand_df.merge(commit_guru_df,on='commit_hash')\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[1:] + [cols[0]]\n",
    "    df = df[cols]\n",
    "    for item in ['Kind', 'Name','commit_hash']:\n",
    "        if item in cols:\n",
    "            df = df.drop(labels = [item],axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv('data/converted/'+ project + '_understand.csv',index=False)\n",
    "#     df,cols = apply_cfs(df)\n",
    "    y = df.Bugs\n",
    "    X = df.drop('Bugs',axis = 1)\n",
    "    cols = X.columns\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    return X,y\n",
    "def apply_smote(df):\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def apply_cfs(df):\n",
    "        y = df.Bugs.values\n",
    "        X = df.drop(labels = ['Bugs'],axis = 1)\n",
    "        X = X.values\n",
    "        selected_cols = CFS.cfs(X,y)\n",
    "        cols = df.columns[[selected_cols]].tolist()\n",
    "        cols.append('Bugs')\n",
    "        return df[cols],cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x,df):\n",
    "    lo = df.min()\n",
    "    hi = df.max()\n",
    "    return (x - lo) / (hi - lo +0.00000001)\n",
    "\n",
    "def dominate(_df,t,row_project_name,goals):\n",
    "    wins = 0\n",
    "    for i in range(_df.shape[0]):\n",
    "        project_name = _df.index[i]\n",
    "        row = _df.iloc[i].tolist()\n",
    "        if project_name != row_project_name:\n",
    "            if dominationCompare(row, t,goals,_df):\n",
    "                wins += 1\n",
    "    return wins\n",
    "\n",
    "def dominationCompare(other_row, t,goals,df):\n",
    "    n = len(goals)\n",
    "    weight = {'recall':1,'precision':1,'pf':-2.5}\n",
    "    sum1, sum2 = 0,0\n",
    "    for i in range(len(goals)):\n",
    "        _df = df[goals[i]]\n",
    "        w = weight[goals[i]]\n",
    "        x = t[i]\n",
    "        y = other_row[i]\n",
    "        x = norm(x,_df)\n",
    "        y = norm(y,_df)\n",
    "        sum1 = sum1 - math.e**(w * (x-y)/n)\n",
    "        sum2 = sum2 - math.e**(w * (y-x)/n)\n",
    "    return sum1/n < sum2/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_self(project):\n",
    "    X,y = load_data(project)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=18)\n",
    "    df_smote = pd.concat([X_train,y_train],axis = 1)\n",
    "    df_smote = apply_smote(df_smote)\n",
    "    y_train = df_smote.Bugs\n",
    "    X_train = df_smote.drop('Bugs',axis = 1)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train,y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    abcd = metrices.measures(y_test,predicted)\n",
    "    pf = abcd.get_pf()\n",
    "    recall = abcd.calculate_recall()\n",
    "    precision = abcd.calculate_precision()\n",
    "    f1 = abcd.calculate_f1_score()\n",
    "    g_score = abcd.get_g_score()\n",
    "    auc = roc_auc_score(y_test, predicted)\n",
    "    print(classification_report(y_test, predicted))\n",
    "    return recall,precision,pf,f1,g_score,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_df = pd.read_csv('projects.csv')\n",
    "projects = proj_df.repo_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = {}\n",
    "recall_list = {}\n",
    "pf_list = {}\n",
    "f1_list = {}\n",
    "g_list = {}\n",
    "auc_list = {}\n",
    "for project in projects:\n",
    "    try:\n",
    "        if project == '.DS_Store':\n",
    "            continue\n",
    "    #     if project != 'guice':\n",
    "    #         continue\n",
    "        print(\"+++++++++++++++++   \"  + project + \"  +++++++++++++++++\")\n",
    "        recall,precision,pf,f1,g_score,auc = run_self(project)\n",
    "        recall_list[project] = recall\n",
    "        precision_list[project] = precision\n",
    "        pf_list[project] = pf\n",
    "        f1_list[project] = f1\n",
    "        g_list[project] = g_score\n",
    "        auc_list[project] = auc\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "final_result = {}\n",
    "final_result['precision'] = precision_list\n",
    "final_result['recall'] = recall_list\n",
    "final_result['pf'] = pf_list\n",
    "final_result['f1'] = f1_list\n",
    "final_result['g'] = g_list\n",
    "final_result['auc'] = auc_list\n",
    "with open('data/self_100.pkl', 'wb') as handle:\n",
    "    pickle.dump(final_result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_precision = list(precision_list.values())\n",
    "_recall = list(recall_list.values())\n",
    "_pf = list(pf_list.values())\n",
    "_f1 = list(f1_list.values())\n",
    "_g = list(g_list.values())\n",
    "_auc = list(auc_list.values())\n",
    "print(np.median(_precision),np.median(_recall),np.median(_pf),np.median(_f1),np.median(_g),np.median(_auc))\n",
    "fig = plt.figure(num=None, figsize = (20,4), facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(161)\n",
    "ax.boxplot(_precision)\n",
    "ax.set_title('Precision',size = 15)\n",
    "ax = fig.add_subplot(162)\n",
    "ax.boxplot(_recall)\n",
    "ax.set_title('Recall',size = 15)\n",
    "ax = fig.add_subplot(163)\n",
    "ax.boxplot(_pf)\n",
    "ax.set_title('pf',size = 15)\n",
    "ax = fig.add_subplot(164)\n",
    "ax.boxplot(_f1)\n",
    "ax.set_title('f1',size = 15)\n",
    "ax = fig.add_subplot(165)\n",
    "ax.boxplot(_g)\n",
    "ax.set_title('g',size = 15)\n",
    "ax = fig.add_subplot(166)\n",
    "ax.boxplot(_auc)\n",
    "ax.set_title('auc',size = 15)\n",
    "# fig.savefig('without_process.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self(project):\n",
    "    X,y = load_data(project)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=18)\n",
    "    df_smote = pd.concat([X_train,y_train],axis = 1)\n",
    "    df_smote = apply_smote(df_smote)\n",
    "    y_train = df_smote.Bugs\n",
    "    X_train = df_smote.drop('Bugs',axis = 1)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = {}\n",
    "recall_list = {}\n",
    "pf_list = {}\n",
    "f1_list = {}\n",
    "g_list = {}\n",
    "auc_list = {}\n",
    "for s_project in projects:\n",
    "    try:\n",
    "        if project == '.DS_Store':\n",
    "            continue\n",
    "    #     if project != 'guice':\n",
    "    #         continue\n",
    "        print(\"+++++++++++++++++   \"  + s_project + \"  +++++++++++++++++\")\n",
    "        clf = run_self(s_project)\n",
    "        if s_project not in precision_list.keys():\n",
    "            precision_list[s_project] = {}\n",
    "            recall_list[s_project] = {}\n",
    "            pf_list[s_project] = {}\n",
    "            f1_list[s_project] = {}\n",
    "            g_list[s_project] = {}\n",
    "            auc_list[s_project] = {}    \n",
    "        for d_project in projects:\n",
    "            try:\n",
    "                X,y = load_data(d_project)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=18)\n",
    "                predicted = clf.predict(X_test)\n",
    "                abcd = metrices.measures(y_test,predicted)\n",
    "                pf = abcd.get_pf()\n",
    "                recall = abcd.calculate_recall()\n",
    "                precision = abcd.calculate_precision()\n",
    "                f1 = abcd.calculate_f1_score()\n",
    "                g_score = abcd.get_g_score()\n",
    "                auc = roc_auc_score(y_test, predicted)\n",
    "                precision_list[s_project][d_project] = precision\n",
    "                recall_list[s_project][d_project] = recall\n",
    "                pf_list[s_project][d_project] = pf\n",
    "                f1_list[s_project][d_project] = f1\n",
    "                g_list[s_project][d_project] = g_score\n",
    "                auc_list[s_project][d_project] = auc\n",
    "            except Exception as e:\n",
    "                print('d_project',e)\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print('s_project',e)\n",
    "        continue\n",
    "final_result = {}\n",
    "final_result['precision'] = precision_list\n",
    "final_result['recall'] = recall_list\n",
    "final_result['pf'] = pf_list\n",
    "final_result['f1'] = f1_list\n",
    "final_result['g'] = g_list\n",
    "final_result['auc'] = auc_list\n",
    "with open('data/conv_bellwether_100.pkl', 'wb') as handle:\n",
    "    pickle.dump(final_result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_self = pd.read_pickle('data/self_100.pkl')\n",
    "df_bellwether = pd.read_pickle('data/conv_bellwether_100.pkl')\n",
    "metrices = df_self.keys()\n",
    "median = 0\n",
    "median_results = {}\n",
    "for s_project in projects:\n",
    "    try:\n",
    "        if s_project not in median_results.keys():\n",
    "            median_results[s_project] = {}\n",
    "        for metric in metrices:\n",
    "            median_results[s_project][metric] = np.median(list(df_bellwether[metric][s_project].values()))\n",
    "    except Exception as e:\n",
    "        print(s_project)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(median_results,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result/cdom_score_100.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_score = []\n",
    "goals = ['recall','precision','pf']\n",
    "for row_id in range(df.shape[0]):\n",
    "    project_name = df.index[row_id]\n",
    "    row = df.iloc[row_id].tolist()\n",
    "    wins = dominate(df,row,project_name,goals)\n",
    "    dom_score.append(wins)\n",
    "df['wins'] = dom_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['wins'] == df.wins.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_self = pd.read_pickle('data/self_100.pkl')\n",
    "df_bellwether = pd.read_pickle('data/conv_bellwether_100.pkl')\n",
    "metrices = df_self.keys()\n",
    "median = 0\n",
    "bell_results = {}\n",
    "for s_project in projects:\n",
    "    try:\n",
    "        for metric in metrices:\n",
    "            if metric not in bell_results.keys():\n",
    "                bell_results[metric] = {}\n",
    "            if s_project not in bell_results[metric]:\n",
    "                bell_results[metric][s_project] = {}\n",
    "            _self = df_self[metric][s_project]\n",
    "            _bell = df_bellwether[metric]['disruptor'][s_project]\n",
    "            bell_results[metric][s_project]['self'] = _self\n",
    "            bell_results[metric][s_project]['bell'] = _bell\n",
    "    except Exception as e:\n",
    "        print(s_project)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrices:\n",
    "    count = 0\n",
    "    for key in bell_results[metric].keys():\n",
    "        try:\n",
    "            if  bell_results[metric][key]['self'] <= bell_results[metric][key]['bell']:\n",
    "                count += 1\n",
    "        except:\n",
    "            continue\n",
    "    print(metric, \"wins\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bell_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
